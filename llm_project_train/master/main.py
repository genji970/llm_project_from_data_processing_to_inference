"""

penai api key envì— ì„¤ì •

os.environ["GOOGLE_API_KEY"] = "your-google-api-key"
os.environ["GOOGLE_CSE_ID"] = "your-custom-search-engine-id"

"""

import os
import argparse
import torch
from llm_project_train.master.arg_add import *
from llm_project_train.master.config import *

# ëª…ë ¹ì¤„ íŒŒì‹±
args = parser.parse_args()
fine_tuning = args.fine_tuning

from llm_project_train.Data_generating.Data_from_web_paper.data_process import data_main
from llm_project_train.Data_generating.Data_from_web_paper.Data_parsing import parsing_2
from llm_project_train.Data_generating.data_to_token import data_to_token_main

from llm_project_train.model_build.model_build_main import *

# âœ… ì €ì¥í•  í´ë” ê²½ë¡œ ì„¤ì •
save_directory = "./saved_model"
os.makedirs(save_directory, exist_ok=True)

# âœ… ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ì €ì¥ ê²½ë¡œ ì§€ì •
model_save_path = os.path.join(save_directory, "full_llm_model.pth")  # ëª¨ë¸ ì „ì²´ ì €ì¥ (êµ¬ì¡° + ê°€ì¤‘ì¹˜)
tokenizer_save_path = save_directory  # í† í¬ë‚˜ì´ì € ì €ì¥ (ë””ë ‰í† ë¦¬ ë‹¨ìœ„)

try:
    # âœ… ëª¨ë¸ ì „ì²´ ì €ì¥ (êµ¬ì¡° + ê°€ì¤‘ì¹˜ í¬í•¨)
    torch.save(llm_model, model_save_path)

    # âœ… í† í¬ë‚˜ì´ì € ì €ì¥
    llm_model.base_model.tokenizer = tokenizer
    llm_model.tokenizer.save_pretrained(tokenizer_save_path)

    # âœ… ì €ì¥ëœ íŒŒì¼ í™•ì¸
    saved_files = os.listdir(save_directory)
    required_files = ["full_llm_model.pth", "tokenizer_config.json", "vocab.json", "merges.txt"]

    missing_files = [file for file in required_files if file not in saved_files]

    if not missing_files:
        print("âœ… ëª¨ë¸ & í† í¬ë‚˜ì´ì € ì €ì¥ ì„±ê³µ!")
        print(f"ğŸ“ ì €ì¥ëœ íŒŒì¼ ëª©ë¡: {saved_files}")
    else:
        print(f"âŒ ì˜¤ë¥˜: ì¼ë¶€ ì €ì¥ íŒŒì¼ì´ ëˆ„ë½ë¨: {missing_files}")

except Exception as e:
    print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")



